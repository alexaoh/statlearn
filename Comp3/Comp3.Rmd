---
title: "Compulsory Exercise 3"
author: Martina Hall, Michail Spitieris, Stefanie Muff, Department of Mathematical
  Sciences, NTNU
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    toc: no
    toc_depth: '2'
subtitle: TMA4268 Statistical Learning V2020
urlcolor: blue
---

<!-- rmarkdown::render("RecEx2-sol.Rmd","all",encoding="UTF-8") -->
<!-- rmarkdown::render("RecEx2-sol.Rmd","html_document",encoding="UTF-8") -->
<!-- rmarkdown::render("RecEx2-sol.Rmd","pdf_document",encoding="UTF-8") -->

Last changes: 13.04.2020

---


```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3,fig.align = "center")
```

# Problem 1 (9P)

In compulsory exercise 2 we used the `College` data from the `ISLR` library, where we wanted to predict Outstate. 

```{r}
library(ISLR)
library(keras)
set.seed(1)
College$Private = as.numeric(College$Private)
train.ind = sample(1:nrow(College), 0.5 * nrow(College))
college.train = College[train.ind, ]
college.test = College[-train.ind, ]
str(college.train)
```

The task here is to fit densely connected neural networks using the package `keras` in order to predict `Outstate`.

## a) (2P)

Preprocessing is important before we fit a neural network. Apply feature-wise normalization to the predictors (but not to the response!).

```{r}
# Feature-wise normalization added to the predictors (not the response).
train.target <- college.train$Outstate
college.train <- subset(college.train, select = -c(Outstate))
test.target <- college.test$Outstate
college.test <- subset(college.test, select = -c(Outstate))

mean <- apply(college.train, 2, mean)
std <- apply(college.train, 2, sd)
college.train <- scale(college.train, center = mean, scale = std)
college.test <- scale(college.test, center = mean, scale = std)
```

## b) (2P)

Write down the equation which describes a network that predicts `Outstate` with 2 hidden layers and `relu` activation function with 64 units each. 
What activation function will you choose for the output layer?

The equation which describes a network that predicts `Outstate` with 2 hidden layers and `relu` activation function with 64 units each, is 

$$
\hat{y}_1({\bf x})=\beta_{01}+\sum_{m=1}^{64} \beta_{m1}\max(\gamma_{0m}+\sum_{l=1}^{64} \gamma_{lm}\max(\alpha_{0l} + \sum_{j=1}^{17}\alpha_{jl}x_j,0),0)
$$

Since `Outstate` is a continuous variable, I would use a linear activation function for the output layer (for regression). 

## c) (3P)

(i) Train the network from b) for the training data using the library `keras`; use 20% of the training data as your validation subset (1P). 

```{r}
set.seed(123)
# Build the model
model <- keras_model_sequential() %>% 
  layer_dense(units = 64, activation = 'relu', input_shape = ncol(college.train)) %>%  
  layer_dense(units = 64, activation = 'relu') %>%
  layer_dense(units = 1) 

model %>% compile(optimizer ='rmsprop',loss ='mse',metrics =c('mae'))

# Train
history <-  model %>% fit(college.train, train.target, epochs = 300, batch_size = 8, validation_split = 0.2, verbose = 0)
```

(ii) Plot the training and validation error as a function of the epochs (1P).

```{r}
plot(history)
```

(iii) Report the MSE of the test set and compare it with methods that you used in Compulsory 2 (1P).

```{r}
result <- model %>% evaluate(college.test, test.target, verbose = 0)
result[1]
```

## d) (2P)

Apply one of the regularization techniques you heard about in the course (easiest to use dropout or weight decay with L1/L2 norms). Does this improve the performance of the network? Please again use `set.seed(123)` to make results comparable.

```{r}
set.seed(123)

# Regularization with weight decay with l2.
# Build the model
model.kernel.reg <- keras_model_sequential() %>% 
  layer_dense(units = 64, activation = 'relu', input_shape = ncol(college.train), 
              kernel_regularizer = regularizer_l2(l = 0.001)) %>%  
  layer_dense(units = 64, activation = 'relu', 
              kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_dense(units = 1) 

model.kernel.reg %>% compile(optimizer ='rmsprop',loss ='mse',metrics =c('mae'))

# Train
history.kernel.reg <-  model.kernel.reg %>% fit(college.train, train.target, epochs = 300, batch_size = 8, validation_split = 0.2, verbose = 0)

# Result for weight decay with l1.
result.kernel.reg <- model.kernel.reg %>% evaluate(college.test, test.target, verbose = 0)
result.kernel.reg[1]

# Regularization with dropout.
# Build the model
model.drop.reg <- keras_model_sequential() %>% 
  layer_dense(units = 64, activation = 'relu', input_shape = ncol(college.train)) %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 64, activation = 'relu') %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 1) 

model.drop.reg %>% compile(optimizer ='rmsprop',loss ='mse',metrics =c('mae'))

# Train
history.drop.reg <-  model.drop.reg %>% fit(college.train, train.target, epochs = 300, batch_size = 8, validation_split = 0.2, verbose = 0)

# Result for dropout..
result.drop.reg <- model.drop.reg %>% evaluate(college.test, test.target, verbose = 0)
result.drop.reg[1]

par(mfrow = c(1,2))
plot(history.kernel.reg)
plot(history.drop.reg)
par(mfrow=c(1,1)) # reset plotting grid. 
```

Both regularization methods seem to improve the model fit by some amount, compared to no use of regularization techniques. The differences are not dramatic however.  

# Problem 2 (10P)

In this problem, we will use a real dataset of individuals with the Covid-19 infection. The data were downloaded from <https://www.kaggle.com/shirmani/characteristics-corona-patients> on 30. March 2020, and have only been cleaned for the purpose of this exercise. The dataset consists of 2010 individuals and four columns, 

  * `deceased`: if the person died of corona (1:yes, 0:no)
  
  * `sex`: male/female 
  
  * `age`: age of person (ranging from 2 years to 99 years old)
  
  * `country`: which country the person is from (France, Japan, Korea or Indonesia)
  
Note that the conclusions we will draw here are probably not scientifically valid, because we do not have enough information about how data were collected.

Load your data into R using the following code:
```{r}
id <- "1CA1RPRYqU9oTIaHfSroitnWrI6WpUeBw" # google file ID
d.corona <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id),header=T)
```


## a) Inspecting your data (1P)

Inspect the data by reporting __tables__ for 

  * the number of deceased for each country, 
  * the number of deceased for each sex, and 
  * for each country: the number of deceased, separate for each sex. 

```{r}
table(d.corona$country, d.corona$deceased)
table(d.corona$sex, d.corona$deceased)

France <-  d.corona[which(d.corona$country=="France"), ]
Japan <- d.corona[which(d.corona$country=="japan"), ]
Korea <- d.corona[which(d.corona$country=="Korea"), ]
Indonesia <- d.corona[which(d.corona$country=="indonesia"), ]

table(France$sex, France$deceased)
table(Japan$sex, Japan$deceased)
table(Korea$sex, Korea$deceased)
table(Indonesia$sex, Indonesia$deceased)
```


## b) Multiple choice (2P)

Answer the following multiple choice questions by using the data above to model the probability of deceased as a function of `sex`, `age` and `country` (with France as reference level; no interactions).

Which of the following statements are true, which false? 

```{r}
glm.fit <- glm(deceased~., family = "binomial", data = d.corona)
summary(glm.fit)
anova(glm.fit, test = "Chisq")
```


(i) Country is not a relevant variable in the model. 

*Answer:* FALSE

(ii) The slope for indonesia has a large $p$-value, which shows that we should remove the Indonesian population from the model, as they do not fit the model as well as the Japanese population.

*Answer:* FALSE

(iii) Increasing the age by 10 years, $x_{age}^* = x_{age}+10$, and holding all other covariates constant, the odds ratio to die increases by a factor of 1.97.

*Answer:* FALSE \textcolor{red}{Explanation:} This is the case because the odds ratio is constantly equal to $\exp{10 \cdot \beta_{sex}}$, while the odds _changes_ with this amount (the odds ratio), when $x$ is changed by 10. 

(iv) The probability to die is approximately $3.12$ larger for males than for females.

*Answer:* FALSE

## c) (1P)

Create a plot of probabilities to die of coronavirus as a function of age, separately for the two sexes and each country. 

Hints: 

* Make one plot and add lines for each country/sex. 
* A useful function to generate gridded data for prediction is `expand.grid()`. For example `newdata = expand.grid(sex="male",age= seq(20,100,1) ,country="France")` generates a grid for males in France over a range of ages between 20 and 100.

```{r}
x.age <- seq(20, 100, 1)
y.male.france <- predict(glm.fit, newdata = expand.grid(sex = "male", age = x.age, country = "France"), type = "response")
y.female.france <- predict(glm.fit, newdata = expand.grid(sex = "female", age = x.age, country = "France"), type = "response")
y.male.japan <- predict(glm.fit, newdata = expand.grid(sex = "male", age = x.age, country = "japan"), type = "response")
y.female.japan <- predict(glm.fit, newdata = expand.grid(sex = "female", age = x.age, country = "japan"), type = "response")
y.male.indonesia <- predict(glm.fit, newdata = expand.grid(sex = "male", age = x.age, country = "indonesia"), type = "response")
y.female.indonesia <- predict(glm.fit, newdata = expand.grid(sex = "female", age = x.age, country = "indonesia"), type = "response")
y.male.korea <- predict(glm.fit, newdata = expand.grid(sex = "male", age = x.age, country = "Korea"), type = "response")
y.female.korea <- predict(glm.fit, newdata = expand.grid(sex = "female", age = x.age, country = "Korea"), type = "response")


plot(x.age, y.male.france, type = "l", ylab = "P(death)", xlab = "Age")
lines(x.age, y.female.france, lty = 2, col = "black")
lines(x.age, y.male.japan, col = "red")
lines(x.age, y.female.japan, lty = 2, col = "red")
lines(x.age, y.male.indonesia, col = "blue")
lines(x.age, y.female.indonesia, lty = 2, col = "blue")
lines(x.age, y.male.korea, col = "green")
lines(x.age, y.female.korea, lty = 2, col = "green")
```


## d) (3P)

As a statistician working on these data, you are asked the following questions: 

  (i) Have males generally a higher probability to die of coronavirus than females?
  
As shown from the full model in b), males do have a higher probability to die than females, since the odds for males is higher. 

  (ii) Is age a greater risk factor for males than for females?

```{r}
fit2 <-  glm(deceased ~ . + age:sex, data = d.corona, family="binomial")
summary(fit2)
```

We cannot conclude that this is the case, since the interaction between sex and age is not significant. Thus, we have no evidence that age is a greater risk for males than females, since the simple null hypothesis for the coefficient of the interaction term cannot be discarded.  

  (iii) Is age a greater risk factor for the French population than for the Korean population?
  
```{r}
fit3 <-  glm(deceased ~ . + age:country, data = d.corona, family="binomial")
summary(fit3)
```

The same conclusion as in (ii): We cannot conclude that this is the case, since the interaction between Korea and age is not significant to any logical level. 

Answer the questions by fitting appropriate models (1P each). 

## e) Interpret your model (1P)

According to your model fitted in part b), it looks like the French population is at a much higher risk of dying from Covid-19 than the other countries. Do you trust this result? How could it be influenced by the way the data were collected? 

No, I think this sounds suspicious. I imagine that there has been run tests on more severe cases in France compared to the ones run in the other cases, i.e. that people in France had more severe symptoms before testing. 

## f) Multiple choice (2P)

Which of the following statements are true, which false? 

Consider the classification tree below to answer:

(i) The probability of dying (`deceased = 1`) is about 0.46 for a French person with age above 91. 

*Answer:* TRUE

(ii) Age seems to be a more important predictor for mortality than sex.

*Answer:* TRUE

Consider the LDA code and output below:

(iii) The "null rate" for misclassification is 2.24%, because this is the proportion of deaths among all cases in the dataset. No classifier should have a higher misclassification rate.

*Answer:* TRUE (even though I get the null rate: `r sum(d.corona$deceased == 1)/sum(d.corona$deceased == 0) * 100` \%).

(iv) LDA is not a very useful method for this dataset, among other reasons because it does not estimate probabilities, but also because the misclassification error is too high.

*Answer:* FALSE


```{r, eval = T, echo=F,fig.width=6,fig.height=5,out.width="60%"}
library(tree)
d.corona$deceased = as.character(d.corona$deceased)
t = tree(deceased ~ age + country + sex , data=d.corona, split = "deviance",control = tree.control(2010, mincut = 5, minsize = 10, mindev = 0.005))
plot(t)
text(t, pretty=0)
```

```{r, eval = T, echo=T}
library(MASS)
table(predict=predict(lda(deceased ~ age + sex + country, data=d.corona))$class,true = d.corona$deceased)
```

 
# Problem 3 (14P)
 

The `d.support` dataset (source _F. E. Harrell, Regression Modeling Strategies_) contains the total hospital costs of 9105 patients with certain diseases in American hospitals between 1989 and 1991. The different variables are

Variable Meaning
------- -------
`totcst`   Total costs 
`age`      Age of the patients 
`dzgroup` Disease group  
`num.co`   Number of co-morbidities
`edu`     Years of education
`scoma`   Measure for Glasgow coma scale
`income`  Income 
`race`    Rasse 
`meanbp`  Mean blood pressure
`hrt`     Heart rate
`resp`    Respiratory frequency
`temp`    Body temperature
`pafi`    PaO2/FiO2 proportion (blood-gas mixture)

Data are loaded as follows (and we reduce the number of patients to the 4960 complete cases with total costs larger than 0):

```{r}
id <- "1heRtzi8vBoBGMaM2-ivBQI5Ki3HgJTmO" # google file ID
d.support <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id),header=T)
# We only look at complete cases
d.support <- d.support[complete.cases(d.support),]
d.support <- d.support[d.support$totcst >0 ,]
```

We would like to build models that help us to understand which predictors are mostly driving the total cost, but also models for prediction. 

## a) (1P)

Before we start analyzing the data, visualize the distributions of all continuous or integer variables with histograms. Suggest a transformation for the response variable `totcst` (hint: it is a _standard transformation_ that we have used earlier in the course). Important: **you should fit all models with the transformed version of the response variable `totcst` from now on. Leave all other variables untransformed.** 

```{r}
hist(d.support$totcst)
hist(log(d.support$totcst))
hist(d.support$age)
hist(d.support$hrt)
hist(d.support$edu)
hist(d.support$meanbp)
hist(d.support$scoma)
hist(d.support$num.co)
hist(d.support$temp)
hist(d.support$resp)
hist(d.support$pafi)
```

A log transform will be used since the distribution of totcst is skewed. 

## b) (3P)

Fit a multiple linear regression model with the six covariates `age`, `temp`, `edu`, `resp`, `num.co` and `dzgroup` and the (transformed version of the) response `totcst`. 

```{r}
linear.fit <- lm(log(totcst) ~ age + temp + edu + resp + num.co + dzgroup, data = d.support)
summary(linear.fit)
```


(i) How much/by which factor are the total costs expected to change when a patient's age increases by 10 years, given that all other characteristica of the patient are the same? Use the transformed respose to fit the model, but report the result on the original (back-transformed) scale of the response. (1P)

*Answer:* When a patient's age increases by 10 years, the total costs are expected to be reduced with $\exp{(10 \cdot -0.0069950)} \approx 0.932$ 

(ii) Do a residual analysis using the Tukey-Anscombe plot and the QQ-diagram. Are the assumptions fulfilled? (1P)

```{r}
library(ggfortify)
autoplot(linear.fit, which = c(1,2))
```

The assumptions of the linear model are fulfilled, when based on the two diagrams above, since the residual plot shows no clear pattern, while the QQ-diagram seems to behave nicely.

(iii) Does the effect of age depend on the disease group? Do a formal test and report the $p$-value. (1P)

```{r}
dzgroup.age <- lm(log(totcst) ~ temp + edu + resp + num.co + dzgroup * age, data = d.support)
anova(dzgroup.age)
```

Yes, the $p$-value of the interaction term between dzgroup and age is 0.0002019, which is significant to a reasonable level. Hence, the effect of age depends on the disease group. 

## c) (3P)

In order to build a more robust model for inference and prediction of the total costs, continue using ridge regression. 
Create a training set with 80% of the data and a test set with the remaining 20% (1P). Run cross-validation to find the largest value of $\lambda$ such that the error is within 1 standard error of the smallest $\lambda$ (1P). Report the test MSE of the ridge regression where you used the respective $\lambda$ (1P).

Be careful: we still use the same transformation for the response as in b) -- you should report the MSE using the transformed version of `totcst` (i.e., do **not back-transform** the MSE to the original scale).

```{r}
library(glmnet)
set.seed(12345)
train.ind = sample(1:nrow(d.support), 0.8*nrow(d.support))
d.support.train = d.support[train.ind,]
d.support.test = d.support[-train.ind,]

x.train <- model.matrix(log(totcst) ~ ., data = d.support.train)[, -1]
y.train <- log(d.support.train$totcst)
x.test <- model.matrix(log(totcst) ~ ., data = d.support.test)[, -1]
y.test<- log(d.support.test$totcst)

set.seed(4268)
cv.ridge <- cv.glmnet(x.train, y.train, alpha = 0)
plot(cv.ridge)
(lambda.ridge <- cv.ridge$lambda.1se)
ridge <- glmnet(x.train, y.train, alpha = 0, lambda = lambda.ridge)
coef(ridge)
plot(glmnet(x.train, y.train, alpha = 0), "lambda")
ridge.pred <- predict(ridge, s = lambda.ridge, newx = x.test)
# MSE 
mean((ridge.pred - y.test)^2)
```

The MSE is reported above. 

## d) (3P)

Now assume that our sole aim is prediction. In the course you heard about  _partial least squares (PLS)_. It is a smart approach that uses the principal component regression idea, but finds the components that are best correlated with the response.

Proceed as follows:

(i) Run a PLS regression (don't forget to scale the variables, `scale=TRUE`) (1P).
(ii) Choose an optimal number of principal components (PCs) using cross-validation (1P).
(iii)  Report the MSE of the test set when using the respective set of PCs and compare to the result from ridge regression. Conclusion? (1P)

```{r}
library(pls)
set.seed(234)
# PLS regression.
pls.fit <- plsr(log(totcst) ~ ., data = d.support.train, scale = TRUE, validation = "CV")
summary(pls.fit)
validationplot(pls.fit, val.type = "MSEP")
# Optimal number of PCs are 4 (Ockham's razor/when choosing the simplest model which is almost optimal)
pls.pred <- predict(pls.fit, d.support.test, ncomp = 4)
mean((pls.pred - log(d.support.test$totcst))^2)
```

The MSE is slightly smaller in this case, compared to the result when using Ridge regression. Thus, PLS is a better choice than Ridge regression, even though the results do not differ by a lot, since prediction is the sole aim in this case. Could also have chosen 6 PCs, since this gives the absolute lowest CV error with the smallest possible model.

## e) (4P)

Now choose two other methods that you know from the course and try to build models with even lower test MSEs than those found so far (imagine that this is a competition where the lowest test MSE wins). Use the same training and test dataset as generated above. And remember that we are still _always_ working with the transformed version of the response variable (`totcst`). In particular, use

(i) One model that involves non-linear transformations of the covariates (e.g., splines, natural splines, polynomials etc) that are combined to a GAM (2P).

```{r}
# GAM.
library(gam)
gam1 <- gam(log(totcst) ~ dzgroup + ns(age, 4) + num.co + race 
            + income + s(edu, 4) + poly(temp, 3)
            + ns(pafi, 5) + poly(scoma, 3)
            + s(meanbp) + ns(hrt, 3) + bs(resp, 5), data = d.support.train)
#plot(gam1)
pred.gam <- predict(gam1, newdata = d.support.test)
mean((pred.gam - log(d.support.test$totcst))^2)
```

(ii) One model/method based on regression trees (2P).

```{r}
# Random Forest.
library(randomForest)
set.seed(4268)
m <- round(ncol(d.support.train)/3) # regression.
trees <- seq(from = 100, to = 800, by = 25)
mses <- rep(0, length(trees))
j <- 1
# for (i in trees){ # Try different amounts of trees B, to see where the error stabilizes. Did not really stabilize.
#   forest.fit <- randomForest(log(totcst) ~ ., data = d.support.train, mtry = m, ntree = i, importance = T)
#   forest.pred <- predict(forest.fit, newdata = d.support.test)
#   mse <- mean((forest.pred - log(d.support.test$totcst))^2)
#   mses[j] <- mse
#   j <- j+1
# } # very slow loop. 
```

```{r}
#plot(trees, mses, type = "l", xlab = "Number of trees B", ylab = "MSE test")
# since they did not really stabilize, choose B = 1000 (e.g.)
train.predictors <- d.support.train[, -7] # remove totcst
y.train <- log(d.support.train[, 7]) # only totcst
test.predictors <- d.support.test[, -7] # remove totcst
y.test <- log(d.support.test[, 7]) # only totcst

# Way better method compared to the foor-loop above! (a lot more effective)
forest.fit <- randomForest(train.predictors, y = y.train, xtest = test.predictors,
                           ytest = y.test, mtry = m, ntree = 1000, importance = T)
plot(1:1000, forest.fit$test$mse, col = "blue", type = "l", ylim = c(0.815, 0.90))
# Choose B = 600 for example.
forest.fit$test$mse[600]
```

Very briefly discuss or explain your choices (1-2 sentences each).

The first choice was a GAM with several different non-linear predictors. These were chosen somewhat randomly in order to reduce the test MSE. 

The second choice was a random forest, since this can be made better that bagging (less correlated trees) in many cases, and is simpler than boosting, which could also have been used. 

# Problem 4 (Mixed questions; 6P)

## a) 2P

We look at the following cubic regression spline model:

$$
Y = 
\left\{
\begin{array}{ll}
 \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3  + \epsilon \ , & \text{ if } \,  x\leq 1 \ , \\
 \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \beta_4 (x-1)^3 + \epsilon  \ , & \text{ if } \,  1 < x\leq 2 \ , \\
 \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \beta_4 (x-1)^3 + \beta_5 (x-2)^3 + \epsilon  \ , &  \text{ if }  \, x>2 \ . \\
\end{array}
\right.
$$

Write down the basis functions (1P) and the design matrix (1P) of this model.

The basis functions of the model are (without the intercept)

$$
X, X^2, X^3, (X-1)_+^3 \text{ and } (X-2)_+^3,
$$
where 

$$
(X-q)_+^3 = \begin{cases}
    (X-q_j)^3 &, X > q_j\\
    0&,\text{otherwise.} 
  \end{cases}
$$

The design matrix of the model is

$$
\begin{pmatrix}
1 & x_1 & x_1^2 & x_1^3 & (x_1-1)_+^3 & (x_1-2)_+^3 \\
1 & x_2 & x_2^2 & x_2^3 & (x_2-1)_+^3 & (x_2-2)_+^3 \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
1 & x_n & x_n^2 & x_n^3 & (x_n-1)_+^3 & (x_n-2)_+^3 
\end{pmatrix}.
$$


## b) Multiple choice - 2P

Inference vs prediction: Which of the following methods are suitable when the aim of your analysis is inference? 

(i) Lasso and ridge regression
(ii) Multiple linear regression with interaction terms
(iii) Logistic regression 
(iv) Support Vector Machines

The suitable methods for inference are (i), (ii) and (iii). SVMs are not suited for inference, since they are hard to interpret. 

## c) Multiple choice - 2P

We again look at the Covid-19 dataset from Problem 2 to study some properties of the bootstrap method. Below we estimated the standard errors of the regression coefficients in the logistic regression model with `sex`, `age` and `country` as predictors using 1000 bootstrap iterations (column `std.error`). These standard errors can be compared to those that we obtain by fitting a single logistic regression model using the `glm()` function. Look at the R output below and compare the standard errors that we obtain from these two approaches (note that the `t1*` to `t6*` variables are sorted in the same way as for the `glm()` output).

```{r}
id <- "1CA1RPRYqU9oTIaHfSroitnWrI6WpUeBw" # google file ID
d.corona <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id),header=T)
```

```{r}
library(boot)
boot.fn <- function(data,index){
  return(coefficients(glm(deceased ~ sex + age + country, family="binomial",data=data,subset=index)))
}
boot(d.corona,boot.fn,1000)
```

```{r}
# Logistic regression
r.glm <- glm(deceased ~ sex + age + country, d.corona,family="binomial")
summary(r.glm)$coef
```

Which of the following statements are true?


(i) There are large differences between the estimated standard errors, which indicates a problem with the bootstrap.

*Answer:* FALSE

(ii) The differences between the estimated standard errors indicate a problem with the assumptions taken about the distribution of the estimated parameters in logistic regression.

*Answer:* TRUE. This is the case because the bootstrap is "always right", since it does not rely on any assumptions. Here, the data points might be dependent, which means that the SE is underestimated in the glm-function and the assumption of independent observation pairs in the logistic regression is broken.

(iii) The `glm` function leads to too small $p$-values for the differences between countries, in particular for the 
differences between Indonesia and France and between Japan and France.

*Answer:* TRUE. This is a consequence of the last point. Since the SE is underestimated, this means that the p-values are too small (since the T-values are too large). 

(iv) The bootstrap relies on random sampling the same data without replacement.

*Answer:* FALSE


# Problem 5 (Multiple and single choice questions; 11P)

## a) Multiple choice - 2P

Which of the following are techniques for regularization?

(i) Lasso
(ii) Ridge regression
(iii) Forward and backward selection
(iv) Stochastic gradient descent

The following are techniques for regularization: (i), (ii) and (iv). Forward and backward selection are not techniques for regularization, since the estimated coefficient are not shrunk (only chosen). 


## b) Multiple choice - 2P

Which of the following statements about principal component regression (PCR) and partial least squares (PLS) are correct?

(i) PCR involves the first principal components that are most correlated with the response.

*Answer:* FALSE

(ii) PLS involves the first principal components that are most correlated with the response.

*Answer:* TRUE

(iii) The idea in PLS is that we choose the principal components that explain most variation among all covariates.

*Answer:* FALSE

(iv) The idea in PCR is that we choose the principal components that explain most variation among all covariates.

*Answer:* TRUE

## c) Single choice - 1P

In ridge regression, we estimate the regression coefficients in a linear regression model by minimizing
$$
\sum_{i=1}^n \left( y_i - \beta_0 - \sum_{j-1}^p \beta_j x_{ij} \right)^2 + \lambda \sum_{j=1}^p \beta_j^2 \ .
$$

What happens when we increase $\lambda$ from 0? Choose the single correct statement:

(i) The training RSS will steadily decrease.
(ii) The test RSS will steadily decrease.
(iii) The test RSS will steadily increase.
(iv) The bias will steadily increase.
(v) The variance of the estimator will steadily increase.

*Answer:* The single correct statement is (iv). 

## d) Single choice - 1P

Which statement about the _curse of dimensionality_ is correct?

(i) It means that we have a bias-variance tradeoff in $K$-nearest neighbor regression, where large $K$ leads to more bias but less variance for the predictor function.
(ii) It means that the performance of the $K$-nearest neighbor classifier gets worse when the number of predictor variables $p$ is large. 
(iii) It means that the $K$-means clustering algorithm performs bad if the datapoints lie in a high-dimensional space.
(iv) It means that support vector machines with radial kernel function should be avoided, because radial kernels correspond to infinite-dimensional polynomial boundaries.
(v) It means that we should never measure too many covariates when we want to do classification.

*Answer:* The single correct statement is (ii).

## e) Single choice - 1P

Now assume you have 10 covariates, $X_1$ to $X_{10}$, each of them uniformly distributed in the interval $[0,1]$. To predict a new test observation $(X^{(0)}_1, \ldots , X^{(0)}_{10})$ in a $K$-nearest neighbor (KNN) clustering approach, we use all observations within 20% of the range closest to each of the covariates (that is, in each dimension). Which proportion of available (training) observations can you expect to use for prediction? 

(i) $1.02 \cdot 10^{-7}$
(ii) $2.0 \cdot 10^{-3}$
(iii) $0.20$
(iv) $0.04$
(v) $10^{-10}$

*Answer:* The single correct statement is (i).

## f) Multiple choice - 2P

This example is taken from a real clinical study by _Ikeda, Matsunaga, Irabu, et al. Using vital signs to diagnose impaired
consciousness: cross sectional observational study. BMJ 2002;325:800_. Researchers investigated the use of vital signs as a screening
test to identify brain lesions in patients with impaired
consciousness. The setting was an emergency department in
Japan. The study included 529 consecutive patients that arrived with consciousness. Patients were
followed until discharge. The vital signs of systolic and diastolic
blood pressure and pulse rate were recorded on arrival. The aim of this study was to find a quick test for assessing whether the newly arrived patient suffered from a brain lesion. 
While vital signs can be measured immediately, the actual diagnosis of a brain lesion can only be determined on the basis of brain imaging and neurological examination at a later stage, thus the quick measurements of blood pressure and heart rate are important to make a quick assessment. In total, 312 patients
(59%) were diagnosed with a brain lesion. 

The performance of each vital sign (systolic blood pressure, diastolic blood pressure and heart rate) was separately evaluated as a screening test to quickly diagnose brain lesions. To assess the quality of each of these vital signs, different thresholds were taken
successively to discriminate between “negative” and
“positive” screening test result. For each vital sign and each threshold the sensitivity and
specificity were derived and used to plot a receiver operating
characteristic (ROC) curve for the vital sign (Figure 1):

\centering
<!-- ![Figure for problem 5f); taken from _P. Sedgwick, BMJ 2011;343_](AUC.png){width=50%} -->

\flushleft

Which of the following statements are true?

(i) The value of 1-specificity represents the proportion of patients without a diagnosed brain lesion identified as positive on screening.
(ii) When we use different cut-offs, sensitivity increases at the cost of lower specificity, and vice versa.
(iii) A perfect diagnostic test has an AUC of 0.5.
(iv) The vital sign that is most suitable to distinguish between patients with and without brain lesion is systolic blood pressure. 

The true statements are (i), (ii) and (iv).

## g) Multiple choice - 2P

We study the `decathlon2` dataset from the `factoextra` package in R, where Athletes' performance during a sporting meeting was recorded. We look at 23 athletes and the results from the 10 disciplines in two competitions. Some rows of the dataset are displayed here:

```{r,eval=T,echo=F}
library(factoextra)
library(FactoMineR)
data("decathlon2")
decathlon2.active <- decathlon2[1:23, 1:10]
names(decathlon2.active) <- c("100m","long_jump","shot_put","high_jump","400m","110.hurdle","discus","pole_vault","javeline","1500m")
```

```{r}
decathlon2.active[c(1,3,4),]
```

From a principal component analysis we obtain the biplot given in Figure 2.

```{r biplot,eval=T,echo=F,fig.width=7,fig.height=7,out.width="55%",fig.cap="Figure for question 5g)."}
r.prcomp <- prcomp(decathlon2.active, scale=T)
biplot(r.prcomp)
```

Which of the following statements are true, which false?

(i) The athlete named CLAY seems to be one of the fastest 1500m runners.
(ii) Athletes that are good in 100m tend to be also good in long jump.
(iii) The first principal component has the highest loadings for 100m and long jump.
(iv) 110m hurdle has a very small loading for PC2.

The true statements are (ii), (iii) and (iv). 
