---
title: "Module 8: Recommended Exercises"
author: "alexaoh"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  pdf_document:
    df_print: kable
subtitle: Statistical Learning V2021
---

```{r setup, include=FALSE}
showsolA<-TRUE
showsolB<-TRUE
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=68),tidy=TRUE)
knitr::opts_chunk$set(echo = TRUE,eval = TRUE, tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize", comment = "#>")
```

## Problem 1 -- Theoretical

a) Provide a detailed explanation of the algorithm that is used to fit a regression tree. What is different for a classification tree? 

**Answer:** Algorithm: The prediction space is splitted in such a way that the RSS across all regions is smallest \textcolor{red}{(More details will follow, with pruning etc!)}. The difference for a classification tree is the criterion used when making binary splits. In the regression setting the RSS is used, since we have a qualitative response. On the contrary, when building a classification tree, the classification error rate could perhaps be used, i.e. the fraction of training observations that do not belong to the majority class in each region. However, this criterion is not sufficiently sensitive when building classification trees, which is the reason to why two other criterion are used in practice: The Gini-index or cross-entropy. The Gini-index is a measure of the total variance across the $K$ classes. Cross-entropy is defined differently, but is quite similar numerically to the Gini-index. Also, the Gini-index and Cross-entropy are differentiable, which may be useful in numerical optimization. 

Also, the prediction in classification is done with a majority vote or by estimation of the probability that the observation belongs to each class (proportion of points in each region belongs to each class), instead of the mean. 

b) What are the advantages and disadvantages of regression and classification trees?

**Advantages:** Interpretability (nice graphical display), close mirror of human decision-making, easily explained concept, handling of qualitative predictors without dummy variables. 

**Disadvantages:** Generally has worse predictive accuracy compared to other classical methods. 

c) What is the idea behind bagging and what is the role of bootstap? How do random forests improve that idea?

**Answer:** The idea behind bagging is to make use of several consecutive bootstrap samples to build trees. In the end, the predictions from each of these trees are averaged, in order to reduce the variance of the predictions. Random forests improve on that idea by restricting the amount of predictors to be chosen from when splitting the regions, i.e. when building the trees. In this manner the trees become less correlated (since more of the trees are potentially different), which may lead to a further decrease in variance of the predictions. 

d) What is an out-of bag (OOB) error estimator and what percentage of observations are included in an OOB sample? (Hint: The result from RecEx5-Problem 4c can be used)

**Answer:** An OOB error estimator is the average (for regression) or the majority vote (for classification) among the predicted response based on the trees where the given predictor is OOB, i.e. the observation was not used when building a tree using the bootstrap sample. About $\frac13$ of the observations are included in the OOB-sample, because, as the result from RecEx5-Problem 4c shows, the probability that a given observation is a part of a bootstrap sample is $\approx \frac23$. 

e) Bagging and Random Forests typically improve the prediction accuracy of a single tree, but it can be difficult to interpret, for example in terms of understanding which predictors are how relevant. 
How can we evaluate the importance of the different predictors for these methods? 

**Answer:**

## Problem 2 -- Regression (Book Ex. 8)

In the lab, a classification tree was applied to the Carseats data set after converting the variable `Sales` into a qualitative response variable. Now we will seek to predict `Sales` using regression trees and related approaches, treating the response as a quantitative variable.

a) Split the data set into a training set and a test set. (Hint: Use 70% of the data as training set and the rest 30% as testing set)

```{r}
library(ISLR)
data("Carseats")
set.seed(4268)
n = nrow(Carseats)
train = sample(1:n, 0.7*n, replace = F)
test = (1:n)[-train]
Carseats.train = Carseats[train, ]
Carseats.test = Carseats[-train, ]
```

b) Fit a regression tree to the training set using the default parameters for the stopping criterion. Plot the tree, and interpret the results. What test MSE do you obtain?

```{r}
library(tree)
tree.mod = tree(Sales~., data = Carseats.train)
summary(tree.mod)
plot(tree.mod)
text(tree.mod)

# Calculate test MSE.
yhat <- predict(tree.mod, newdata = Carseats.test)
mse <- mean((yhat - Carseats.test$Sales)^2)
mse
```

The results are very hard to interpret because of the relatively "bushy" tree. 

c) Use cross-validation in order to determine an optimal level of tree complexity. Does pruning the tree improve the test MSE?

```{r, eval=F, echo=T}
set.seed(4268)
cv.Carseats  = cv.tree(tree.mod) 
tree.min =  which.min(cv.Carseats$dev)
best = cv.Carseats$size[tree.min]
plot(cv.Carseats$size, cv.Carseats$dev, type = "b")
points(cv.Carseats$size[tree.min], cv.Carseats$dev[tree.min], col = "red", pch = 20)

k.best.tree <- prune.tree(tree.mod, best = 11)
summary(k.best.tree)
plot(k.best.tree)
text(k.best.tree, pretty= 0)

# Calculate test MSE.
yhat2 <- predict(k.best.tree, newdata = Carseats.test)
mse2 <- mean((yhat - Carseats.test$Sales)^2)
mse2

```

d) Use the bagging approach with 500 trees in order to analyze the data. What test MSE do you obtain? Use the `importance()` function to determine which variables are most important.

**R-hints**

```{r, eval=F, echo=T}
library(randomForest)
dim(Carseats)
bag.Carseats = randomForest(Sales~., ... , ntree = 500, importance = TRUE)
```

e) Use random forests and to analyze the data. Include 500 trees and select 3 variables for each split. What test MSE do you obtain? Use the `importance()` function to determine which variables are most important. Describe the effect of m, the number of variables considered at each split, on the error rate obtained.

**R-hints**

```{r, eval=F, echo=T}
rf.Carseats = randomForest(Sales~., ... , mtry = 3, ntree = 500, importance = TRUE)
```


f) Finally use boosting with 500 trees, an interaction depth $d=4$ and a shrinkage factor $\lambda=0.1$ (default in the `gbm()` function) on our data. Compare the MSE to all other methods.

**R-hints**

```{r, eval=F, echo=T}
library(gbm)
r.boost=gbm(Sales~., Carseats.train,
                 distribution=...,
                 n.trees= ... ,interaction.depth= ..., shrinkage = ...)
```

g) What is the effect of the number of trees (`ntree`) on the test error? Plot the test MSE as a function of `ntree` for both the bagging and the random forest method.


## Problem 3 -- Classification

In this exercise you are going to implement a spam filter for e-mails by using tree-based methods. Data from 4601 e-mails are collected and can be uploaded from the kernlab library as follows:
```{r}
library(kernlab)
data(spam)
```
Each e-mail is classified by `type` ( `spam` or `nonspam`), and this will be the response in our model. In addition there are 57 predictors in the dataset. The predictors describe the frequency of different words in the e-mails and orthography (capitalization, spelling, punctuation and so on).

a) Study the dataset by writing `?spam` in R.

b) Create a training set and a test set for the dataset. (Hint: Use 70% of the data as training set and the rest 30% as testing set)

c) Fit a tree to the training data with `type` as the response and the rest of the variables as predictors. Study the results by using the `summary()` function. Also create a plot of the tree. How many terminal nodes does it have?
  
d) Predict the response on the test data. What is the misclassification rate?
  
e) Use the `cv.tree()` function to find an optimal tree size. Prune the tree according to the optimal tree size by using the `prune.misclass()` function and plot the result. Predict the response on the test data by using the pruned tree. What is the misclassification rate in this case?
  
f) Create a decision tree by using the bagging approach with $B=500$. Use the function `randomForest()` and consider all of the predictors in each split. Predict the response on the test data and report the misclassification rate.

g) Apply the `randomForest()` function again with 500 trees, but this time consider only a subset of the predictors in each split. This corresponds to the random forest-algorithm. Study the importance of each variable by using the function `importance()`. Are the results as expected based on earlier results? Again, predict the response for the test data and report the misclassification rate.

h) Use `gbm()` to construct a boosted classification tree using 5000 trees, an interaction depth of $d=3$ and a shrinkage parameter of $\lambda=0.001$. Predict the response for the test data and report the misclassification rate.

i) Compare the misclassification rates in d-h. Which method gives the lowest misclassification rate for the test data? Are the results as expected?
