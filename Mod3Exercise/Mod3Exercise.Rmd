---
title: "Module 3: Recommended Exercises"
author: "alexaoh"
date: "21.01.2021"
output: 
  pdf_document:
    df_print: kable
---

```{r setup, include=FALSE}
showsolA<-TRUE
showsolB<-TRUE
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=68),tidy=TRUE)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize", comment = "#>")
```

# Problem 1 (Extension from Book Ex. 9)

This question involves the use of multiple linear regression on the `Auto` data set from `ISLR` package  (you may use `?Auto` to see a description of the data). First we exclude from our analysis the variable `name` and look at the data summary and structure of the dataset.

```{r, cache = F}
library(ISLR)
Auto = subset(Auto, select = -name)
#Auto$origin = factor(Auto$origin)
summary(Auto)
str(Auto)
```

We obtain a summary and see that all variables are numerical (continuous). However, when we check the description of the data (again with `?Auto`) we immediately see that `origin` is actually encoding for either American (origin=1), European (origin=2) or Janapense (origin=3) origin of the car, thus the values 1, 2 and 3 do not have any actual numerical meaning. We therefore need to first change the data type of that variable to let R know that we are dealing with a qualitative (categorical) variable, instead of a continuous one (otherwise we will obtain wrong model fits). In R such variables are called _factor variables_, and before we continue to do any analyses we first need to convert `origin` into a factor variable (a synonymous for "qualitative predictor"): 

```{r, cache = F}
Auto$origin = factor(Auto$origin) 
```


## a)
Use the function `ggpairs()` from `GGally` package to produce a scatterplot matrix which includes all of the variables in the data set.

```{r}
library(GGally)
ggpairs(Auto)
```

## b) 
Compute the correlation matrix between the variables. You will need to remove the factor covariate `origin`, because this is no longer a continuous variable.

```{r}
variables <- Auto[-c(8)] # Remove 'origin' from the data set. 
Sigma <- cor(variables)
Sigma
```


## c) 
Use the `lm()` function to perform a multiple linear regression with `mpg` (miles per gallon, a measure for fuel consumption) as the response and all other variables (except `name`) as the predictors. Use the `summary()` function to print the results. Comment on the output. In particular:

i. Is there a relationship between the predictors and the response?

ii. Is there evidence that the weight of a car influences `mpg`? Interpret the regression coefficient $\beta_{\text{weight}}$ (what happens if a car weights 1000kg more, for example?).

iii. What does the coefficient for the year variable suggest?

```{r}
mreg <- lm(mpg~., data = Auto) # name has already been removed from Auto. 
summary(mreg)
```

Comments on the output:

i) The F-statistic is 224.5, with a p-value of less than $2.2\mathrm{e}{-6}$, which signals that there is a relationship between the predictors and the response. Moreover, several of the p-values of the coefficients related to the predictors are significant. As noted on page 77 in ISLR, each of these p-values give the partial effect of adding that specific variable to the model, while keeping the others in. However, as also noted, there are some flaws with only noting these values, which is why the F-statistic is of main concern when concluding whether or not the predictors are useful in predicting the response. 

ii) The p-value of the coefficient $\beta_{\text{weight}}$ is $2\mathrm{e}{-16}$ which could be evidence that the weight of a car influences `mpg` (at least it is not evidence against whether or not the weight of a car influences `mpg`). The interpretation of the coefficient is that the `mpg` changes, on average, by $\beta_{\text{weight}} = 6.710\mathrm{e}{-3}$ for every one-valued increase in the weight of the car, given that all the other predictors are fixed. This means that, e.g., if a car weighs 1000kg, the `mpg` is estimated to be reduced by 6.710. 

iii) The coefficient for the year variable suggests that `mpg` is increased by 0.770 for each increase in model year of the car. 

## d)
Look again at the regression output from question c). Now we want to test whether the `origin` variable is important. How does this work for a factor variable with more than only two levels?

We construct dummy variables such that he coefficients can be estimated in the regression. If we have $k$ levels in the factor variable, we construct $k-1$ dummy variables. In this way, a baseline level is made, and each coefficient says something about the difference in the response when each respective category is fulfilled with respect to the baseline level. This will become more clear after the example in this task. 

`Origin2` and `Origin3` are the two factor-coefficients that R made. This means that origin category 1 will be regarded as the baseline, which is given by the interceipt. In this case we can see that both the estimations of European (2) and Japanese (3) are significant (from their p-values) and that they give positive values of the `mpg` when added to the interceipt (the baseline, which is American (1)). Have a look at the R-block below for the dummy variables that R made automatically for the categorical variable. 

```{r}
contrasts(Auto$origin)
```
As one can see, the dummy variables 2 and 3 were made automatically by R, with 1 as a baseline, as predicted. This means that the dummy variable 'Origin2' takes on the value 1 if the the origin is 2 (European) and zero otherwise. Moreover, the dummy variable 'Origin3' takes on the value 1 if the origin is 3 (Japanese) and zero otherwise. The baseline, 1 (American), corresponds to when both dummy variables are zero-valued. Hence, this is given by the interceipt. 

## e) 
Use the `autoplot()` function from the `ggfortify` package to produce diagnostic plots of the linear regression fit by setting `smooth.colour = NA`, as sometimes the smoothed line can be misleading. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage?

```{r}
library(ggfortify)
autoplot(mreg, smooth.colour = NA)
```

Comments on the plots:  
From the residual plot in the upper left it looks like the residuals form a pattern closer to a quadratic, which suggests that there might be some problems with the fit. More specifically, it looks like the variance is increasing with the fitted values, which suggests that the assumption about constant variance is not fulfilled. However, the assumption of 0 expectation of the residuals seems to be fulfilled. The Scale-Location plot suggests what has already been noted as well, that the variance is increasing with increasing fitted value.  
I cannot, however, identify any unusually large outliers in this plot.

Moreover, it looks like there are a few points in each of the categories that are high leverage points, especially in factor level 3. 

The points 323, 326 and 327 especially caught my eye, since they have large residuals and they deviate a lot in the upper right part of the QQ-plot. Also, some of these are marked in the leverage-plat as well. This might suggest that these points are outliers and they might need to be checked for errors in the experiment/data. 

Based on the "QQ-plot" test below, I would say that the QQ-plot does not look too good in this case, since the sample quantiles show a highly deviating trend in the top right of the plot, which looks to be more extreme than in some of the plots below. 

## f)
For beginners, it can be difficult to decide whether a certain QQ plot looks "good" or "bad", because we only look at it and do not test anything. A way to get a feeling for how "bad" a QQ plot may look, even when the normality assumption is perfectly ok, we can use simulations: We can simply draw from the normal distribution and plot the QQ plot. Use the following code to repeat this six times:

```{r, eval=TRUE}
set.seed(2332)
n = 100
par(mfrow = c(2,3))
for(i in 1:6){
  sim = rnorm(n)
  qqnorm(sim, pch = 1, frame = FALSE)
  qqline(sim, col = "blue", lwd = 1)
}
```

The conclusion is that the Q-Q-plot looks good?

## g)
Let us look at interactions. These can be included via the `*` or `:` symbols in the linear predictor of the regression function (see Section 3.6.4 in the course book).

Fit another model for `mpg`, including only `displacement`, `weight`, `year` and `origin` as predictors, plus an interaction between `year` and `origin` (interactions can be included as `year*origin`; this adds the main effects and the interaction at once). Is there evidence that the interactions term is relevant? Give an interpretation of the result. 

```{r}
interactions.fit <- lm(mpg ~ displacement + weight + year*origin, data = Auto)
summary(interactions.fit)
fit.without.interactions <- lm(mpg ~ displacement + weight + year + origin, data = Auto)
summary(fit.without.interactions)

anova(interactions.fit, fit.without.interactions)
```

The R-squared is slightly bigger than the value obtained when only fitting the main effects, while the F-statistic is slightly smaller. Moreover, the interaction terms could be described as significant in the fit with interactions (based on relatively small p-values), but they cannot be used to state the significance of the interaction terms definitely.  However, based on the small differences in R-squared (and F-statistic) between the two fits, I would say that there is no clear evidence that the interaction terms are relevant. On the other hand, the anova results in a relatively small Pr(>F) in my opinion, which might suggest that the fit with the interactions could have some merit. 

## h)
Try a few different transformations of the variables, such as $\log(X),$ $\sqrt{X},$ $X^2$. See Section 3.6.5 in the course book for how to do this. Perhaps you manage to improve the residual plots that you got in e)? Comment on your findings.

```{r}
# log-transformation.
# log <- lm(mpg ~ . + log(weight) + log(year), data = Auto)
# summary(log)
# autoplot(log)

# sqrt-transformation.
#sqrt <- lm(mpg ~ . + sqrt(weight) + sqrt(year), data = Auto)
#summary(sqrt)
#autoplot(sqrt)

# x^2-transformation.
#squared <- lm(mpg ~ . + I(year^2) + I(weight^2), data = Auto)
#summary(squared)
#autoplot(squared)
```


I think that the residual plots seem to gain less patterns when using these transformations on some of the variables, but I think the QQ-plot only gets worse. Also, the same outliers as noted earlier as still present, which further substantiates my suspicion that these points should be checked. 

# Problem 2

## a)
A core finding for the least-squares estimator $\hat{\boldsymbol\beta}$ of linear regression models is
$$ \hat{\boldsymbol\beta}=({\bf X}^T{\bf X})^{-1} {\bf X}^T {\bf Y} \ , $$
with $\hat{\boldsymbol\beta}\sim N_{p}(\boldsymbol\beta,\sigma^2({\bf X}^T{\bf X})^{-1})$.

* Show that $\hat{\boldsymbol\beta}$ has this distribution with the given mean and covariance matrix. 
* What do you need to assume to get to this result? 
* What does this imply for the distribution of the $j$th element of $\hat{\boldsymbol\beta}$? 
* In particular, how can we calculate the variance of $\hat{\beta}_j$?

PROOF: 

* First, we show that $\mathsf{Cov}(\hat{\boldsymbol\beta}) = \sigma^2({\bf X}^T{\bf X})^{-1}$. We begin the proof by defining the quantity ${\bf C}=({\bf X}^T{\bf X})^{-1} {\bf X}^T$ such that $\hat{\boldsymbol\beta} = \bf{CY}$. Now, calculating the covariance matrix gives

$$
\begin{split}
\mathsf{Cov}(\hat{\boldsymbol\beta}) = \mathsf{Cov}({\bf CY}) &= \bf{C}\cdot\mathsf{Cov}({\bf Y})\cdot{\bf C}^T \\
&= ({\bf X}^T{\bf X})^{-1} {\bf X}^T \cdot \sigma^2{\bf I}\cdot ({\bf X}^T{\bf X})^{-1} {\bf X}^T)^T \\
&= \sigma^2\{({\bf X}^T{\bf X})^{-1} {\bf X}^T \cdot {\bf X} ({\bf X}^T{\bf X})^{-T}\} \\
&= \sigma^2\{({\bf X}^T{\bf X})^{-1} \cdot {\bf X}^T {\bf X} ({\bf X}^T{\bf X})^{-1}\} \\
&= \sigma^2({\bf X}^T{\bf X})^{-1}, 
\end{split}
$$

where we assume that $\mathsf{Cov}({\bf Y}) = \sigma^2\bf{I}$. 

Next we show that $\mathsf{E}(\hat{\boldsymbol\beta}) = \boldsymbol{\beta}$. This is shown by regular calculation of multivariate expectation, as done in the following 

$$
\begin{split}
\mathsf{E}(\hat{\boldsymbol\beta}) = \mathsf{E}(\bf{CY}) &= {\bf C}\cdot\mathsf{E}({\bf Y}) = {\bf C}\cdot{\bf X}\boldsymbol{\beta} \\
&= ({\bf X}^T{\bf X})^{-1} {\bf X}^T{\bf X}\boldsymbol{\beta} = \boldsymbol{\beta}, 
\end{split}
$$

where we have assumed that $\mathsf{E}({\bf Y}) = \bf{X}\boldsymbol\beta$. 

Finally, $\hat{\boldsymbol{\beta}}$ has a multivariate normal distribution since it is a linear combination of multivariate normal distributions, assuming that ${\bf Y}$ is normally distributed. Hence, the distribution of $\hat{\boldsymbol{\beta}}$ is given by 

$$\hat{\boldsymbol\beta}\sim N_{p}(\boldsymbol\beta,\sigma^2({\bf X}^T{\bf X})^{-1}).$$

The distribution is of dimension $p$ (here) because of the dimensions of the design matrix. 

* Assumptions: The necessary assumptions are, as shortly noted earlier, that 

$${\bf Y=X \boldsymbol\beta}+{\boldsymbol \varepsilon} \ , \quad \boldsymbol\varepsilon\sim N_n({\bf 0},\sigma^2 {\bf I}) \  $$
which results in 

$$ {\bf Y} \sim N_{n}({\bf X} {\boldsymbol\beta},\sigma^2 {\bf I}).$$

* For the $j^{\text{th}}$ element of $\hat{\boldsymbol\beta}$ this implies that it is (univariate) normally distributed with itself as expected value and the $j^{\text{th}}$ element of $\sigma^2({\bf X}^T{\bf X})^{-1}$ as variance. 

* The variance of $\hat{\beta_j}$ can be found by looking up the $j^{\text{th}}$ (diagonal, $\Sigma_{jj}$) element of $\sigma^2({\bf X}^T{\bf X})^{-1}$. 

## b) 
What is the interpretation of a 95% confidence interval? Hint: repeat experiment (on $Y$), on average how many CIs cover the true $\beta_j$? The following code shows an interpretation of a $95\%$ confidence interval. 

* Model: $Y = 1 + 3X + \varepsilon$, with $\varepsilon \sim \mathsf{N}(0,1)$.

```{r}
beta0 = 1
beta1 = 3
true_beta = c(beta0, beta1) # vector of model coefficients
true_sd = 1 # choosing true sd
X = runif(100,0,1) # simulate the predictor variable X
Xmat = model.matrix(~X, data = data.frame(X)) # create design matrix
ci_int = ci_x = 0 # Counts how many times the true value is within the confidence interval
nsim = 1000
for (i in 1:nsim){
  y = rnorm(n = 100, mean = Xmat%*%true_beta, sd = rep(true_sd, 100))
  mod = lm(y ~ x, data = data.frame(y = y, x = X))
  ci = confint(mod)
  ci_int[i] = ifelse(true_beta[1] > ci[1,1] & true_beta[1] < ci[1,2], 1, 0) # if true value of beta0 is within the CI then 1 else 0
  ci_x[i] = ifelse(true_beta[2] > ci[2,1] & true_beta[2] < ci[2,2], 1, 0) # if true value of beta_1 is within the CI then 1 else 0
}
c(mean(ci_int), mean(ci_x))
```


The interpretation of the 95% confidence interval is: If values are drawn, parameters are estimated and confidence intervals are computed many times (for each of these draws), then 95% of these confidence intervals will contain the true value. As one can see from the simulations, this makes sense. 

## c)
What is the interpretation of a 95% prediction interval? Hint: repeat experiment (on $Y$) for a given ${\boldsymbol x}_0$. Write R code that shows the interpretation of a 95% PI. Hint: In order to produce the PIs use the data point $x_0 = 0.4.$ Furthermore you may use a similar code structure as in b). 

```{r}
beta0 = 1
beta1 = 3
true_beta = c(beta0, beta1) # vector of model coefficients
true_sd = 1 # choosing true sd
X = runif(100,0,1) # simulate the predictor variable X
Xmat = model.matrix(~X, data = data.frame(X)) # create design matrix
pi_count= 0 # Counts how many times the true value is within the prediction interval
nsim = 1000
next.value <- 0.4
for (i in 1:nsim){
  y = rnorm(n = 100, mean = Xmat%*%true_beta, sd = rep(true_sd, 100))
  mod = lm(y ~ x, data = data.frame(y = y, x = X))
  pi = predict(mod, newdata = data.frame(x=next.value), interval = "prediction", type = "response")
  pred.value = 1 + 3*next.value + rnorm(n = 1) # predicted value of unobserved value response
  pi_count[i] = ifelse(pred.value > pi[1,2] & pred.value < pi[1,3], 1, 0) # if true value of beta_1 is within the CI then 1 else 0
}
mean(pi_count)
```

The interpretation of a prediction interval is: If values are drawn, parameters are estimated and a prediction of the next observation is made (for each of these draws) many times, then the true next (un-observed) value will be in 95% of these prediction intervals. As is seen from the code the interpretation of the prediction interval is verified. 

A difference between the confidence intervals and prediction intervals in this case is that the confidence intervals are calculated based one each parameter $\hat{\boldsymbol{\beta}}$ while the prediction interval is only calculated with respect to one predicted response from the linear model. 

## d)
Construct a 95% CI for ${\boldsymbol x}_0^T \beta$. Explain what is the connections between a CI for $\beta_j$, a CI for ${\boldsymbol x}_0^T \beta$ and a PI for $Y$ at ${\boldsymbol x}_0$.

```{r}
# Assuming the same model used above. 
beta0 = 1
beta1 = 3
true_beta = c(beta0, beta1) # vector of model coefficients
true_sd = 1 # choosing true sd
X = rep(0.4, 100)
Xmat = model.matrix(~X, data = data.frame(X)) # create design matrix
y = rnorm(n = 100, mean = Xmat%*%true_beta, sd = rep(true_sd, 100))
mod = lm(y ~ x, data = data.frame(y = y, x = X))
ci = confint(mod)
ci

pi = predict(mod, newdata = data.frame(x=next.value), interval = "prediction", type = "response")
pred.value = 1 + 3*next.value + rnorm(n = 1)
pi


```
Tror ikke at jeg helt har forstÃ¥tt oppgaven!?!?

## e)
Explain the difference between _error_ and _residual_.  What are the properties of the raw residuals? Why don't we want to use the raw residuals for model check? What is our solution to this? 

The _error_ is the squared difference between the predicted value and the true value in the population (summed over all points). This is therefore a theoretical construct, something we can never calculate exactly, since the true value is never known. 

The _residual_ is the difference between the predicted value and the observed value (summed over all points). 

The properties of the raw residuals are ??
We don't want to use the raw residuals for model check because ?? (they are biased?)
Our solution to this is therefore to ...
